{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOatmtwrhlMVfWcIzrneZjg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/edwinmgallego/Best-README-Template/blob/main/T%C3%A9cnicas_de_an%C3%A1lisis_textual_y_NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import fitz  # PyMuPDF\n",
        "\n",
        "# ğŸ”¹ Especifica la ruta del archivo PDF\n",
        "pdf_path = \"/ruta/del/archivo/tu_archivo.pdf\"  # Cambia esto por la ruta real\n",
        "\n",
        "# ğŸ”¹ Abrir el PDF y extraer el texto\n",
        "doc = fitz.open(pdf_path)\n",
        "text = \"\\n\".join(page.get_text(\"text\") for page in doc)\n",
        "\n",
        "# ğŸ”¹ Imprimir el texto extraÃ­do\n",
        "print(text)\n"
      ],
      "metadata": {
        "id": "5MhJCxfCFhG9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Procesamiento de Lenguaje Natural (NLP)\n",
        "\n",
        "* El NLP combina la lingÃ¼Ã­stica computacional con modelos estadÃ­sticos, de aprendizaje automÃ¡tico y aprendizaje profundo para permitir que las computadoras procesen y analicen grandes cantidades de datos de lenguaje natural.\n"
      ],
      "metadata": {
        "id": "yx09FH713Ds9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oa0d9p-n3C9z",
        "outputId": "003ab4b4-241e-4880-b32b-7a084c14a1a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hola', ',', 'Â¿cÃ³mo', 'estÃ¡s', '?', 'Espero', 'que', 'todo', 'bien', '.']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# importando NLTK (Natural Language Toolkit), una librerÃ­a de Python para el procesamiento de lenguaje natural. AdemÃ¡s, desde nltk.tokenize, importas word_tokenize, que se usa para dividir un texto en palabras (tokenizaciÃ³n).\n",
        "\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "# Download the 'punkt_tab' resource\n",
        "#'punkt' es un paquete de NLTK que contiene datos necesarios para la tokenizaciÃ³n de texto en inglÃ©s y otros idiomas.\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "\n",
        "#AquÃ­ word_tokenize(texto) divide el texto en palabras y signos de puntuaciÃ³n, devolviendo una lista de \"tokens\".\n",
        "texto = \"Hola, Â¿cÃ³mo estÃ¡s? Espero que todo bien.\"\n",
        "tokens = word_tokenize(texto)\n",
        "print(tokens)\n",
        "\n",
        "#ğŸ”¹ Observaciones:\n",
        "#El texto se ha separado en palabras y signos de puntuaciÃ³n.\n",
        "#El signo Â¿ es un token independiente.\n",
        "#El punto final . tambiÃ©n es tratado como un token separado.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download es_core_news_sm # Download the Spanish language model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lZ5NbB8F7ICE",
        "outputId": "05f1537a-c91c-4831-af3b-c2ec5693a7ed"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting es-core-news-sm==3.7.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/es_core_news_sm-3.7.0/es_core_news_sm-3.7.0-py3-none-any.whl (12.9 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m12.9/12.9 MB\u001b[0m \u001b[31m62.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.0 in /usr/local/lib/python3.11/dist-packages (from es-core-news-sm==3.7.0) (3.7.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.0.12)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.15.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (4.67.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.10.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.1.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (75.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.5.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.27.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2025.1.31)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.11/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.20.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (7.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.17.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.1.2)\n",
            "Installing collected packages: es-core-news-sm\n",
            "Successfully installed es-core-news-sm-3.7.0\n",
            "\u001b[38;5;2mâœ” Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('es_core_news_sm')\n",
            "\u001b[38;5;3mâš  Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**LematizaciÃ³n**"
      ],
      "metadata": {
        "id": "4hfL3Spb5_dF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Esto importa la biblioteca spaCy, que es una poderosa herramienta para el procesamiento del lenguaje natural (NLP).\n",
        "\n",
        "import spacy\n",
        "\n",
        "\n",
        "#AquÃ­ se carga un modelo de spaCy en espaÃ±ol llamado es_core_news_sm. Este modelo tiene conocimiento sobre la gramÃ¡tica del espaÃ±ol y permite realizar tareas como tokenizaciÃ³n, lematizaciÃ³n, anÃ¡lisis gramatical, reconocimiento de entidades.\n",
        "\n",
        "nlp = spacy.load('es_core_news_sm') # Load the downloaded model\n",
        "doc = nlp(\"Estoy corriendo en el parque\")\n",
        "for token in doc:\n",
        "    print(token.text, token.lemma_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fWqDcg5S6JoZ",
        "outputId": "e82f0462-1f0a-46e6-9662-7fdec3b26335"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Estoy estar\n",
            "corriendo correr\n",
            "en en\n",
            "el el\n",
            "parque parque\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "\n",
        "\n",
        "stop_words = set(stopwords.words('spanish'))\n",
        "filtered_tokens = [word for word in tokens if not word in stop_words]\n",
        "print(filtered_tokens)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EyC-sgli9zwC",
        "outputId": "2935805b-a08a-476a-e846-3fb809b02435"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hola', ',', 'Â¿cÃ³mo', '?', 'Espero', 'bien', '.']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "* TextBlob  es una biblioteca de procesamiento de texto que permite anÃ¡lisis de sentimientos, correcciÃ³n ortogrÃ¡fica, traducciÃ³n, etc.\n",
        "texto = TextBlob(\"Me encanta este lugar, es maravilloso.\")\n",
        "\n",
        "*  Se crea un objeto TextBlob con el texto \"Me encanta este lugar, es maravilloso.\".\n",
        "\n",
        "*  TextBlob analiza el texto y puede extraer informaciÃ³n como polaridad y subjetividad.\n",
        "print(texto.sentiment)\n",
        "\n",
        "* texto.sentiment devuelve un objeto con dos valores:\n",
        "** polarity (polaridad): Un valor entre -1 y 1 que indica si el sentimiento es negativo (-1), neutro (0) o positivo (1).\n",
        "* * subjectivity (subjetividad): Un valor entre 0 y 1 que indica quÃ© tan subjetivo es el texto (0 = objetivo, 1 = subjetivo)."
      ],
      "metadata": {
        "id": "8P6uxxkvA1kY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from textblob import TextBlob\n",
        "\n",
        "\n",
        "\n",
        "texto = TextBlob(\"Me encanta este lugar, es maravilloso.\")\n",
        "print(texto.sentiment)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QhRfVlm4AWDy",
        "outputId": "ed91f1bc-96b3-4a4e-a3f7-1c896210b2bc"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentiment(polarity=0.0, subjectivity=0.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* nlp es un modelo de spaCy previamente cargado (por ejemplo, nlp = spacy.load(\"es_core_news_sm\") si se usa el modelo en espaÃ±ol).\n",
        "\n",
        "* Se procesa el texto \"Apple estÃ¡ buscando comprar una startup del Reino Unido por mil millones de dÃ³lares.\"\n",
        "doc es un objeto de spaCy que contiene el anÃ¡lisis del texto, incluyendo tokens, entidades, dependencias gramaticales, etc.\n",
        "\n",
        "* * doc.ents contiene las entidades nombradas que spaCy ha identificado en el texto.\n",
        "* * El bucle for recorre cada entidad (ent) y muestra:\n",
        "ent.text: el fragmento del texto que corresponde a la entidad.\n",
        "ent.label_: la etiqueta de la entidad (tipo de entidad reconocida).\n",
        "\n",
        "* ExplicaciÃ³n de las etiquetas:\n",
        "ORG â†’ OrganizaciÃ³n (Apple)\n",
        "LOC â†’ UbicaciÃ³n (Reino Unido)\n",
        "MONEY â†’ Cantidad de dinero (mil millones de dÃ³lares)\n",
        "Si no se detectan correctamente, puede ser necesario utilizar un modelo mÃ¡s grande (es_core_news_md o es_core_news_lg) o ajustar el reconocimiento de entidades con entrenamiento adicional.\n",
        "\n",
        "ConclusiÃ³n\n",
        "Este cÃ³digo utiliza spaCy para extraer entidades nombradas (como nombres de empresas, lugares y cantidades de dinero) de un texto en espaÃ±ol y mostrarlas junto con su categorÃ­a."
      ],
      "metadata": {
        "id": "9zHF2JzjB_LZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(\"Apple estÃ¡ buscando comprar una startup del Reino Unido por mil millones de dÃ³lares.\")\n",
        "\n",
        "\n",
        "for ent in doc.ents:\n",
        "    print(ent.text, ent.label_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iuMvmnuQBdm2",
        "outputId": "25002133-07bb-4d63-8b59-761e914aa062"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Apple ORG\n",
            "Reino Unido LOC\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Modelado de temas"
      ],
      "metadata": {
        "id": "sTIKibdHCvl0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* CountVectorizer: Convierte texto en una matriz de conteo de palabras.\n",
        "LatentDirichletAllocation: Implementa el algoritmo LDA para la detecciÃ³n de temas en textos.\n",
        "\n",
        "\n",
        "\n",
        "* n_components=2: Se extraerÃ¡n dos temas principales de los documentos.\n",
        "random_state=42: Se fija una semilla para reproducibilidad.\n",
        "* fit(doc_term_matrix): Se entrena el modelo para identificar patrones de palabras que forman los temas.\n",
        "\n",
        "* lda.components_ contiene los pesos de las palabras en cada tema.\n",
        "topic.argsort()[-10:]: Obtiene las 10 palabras mÃ¡s representativas del tema actual (ordenadas por importancia).\n",
        "\n",
        "* Resumen:\n",
        "Convierte texto a una matriz numÃ©rica.\n",
        "Usa LDA para extraer temas latentes.\n",
        "Imprime palabras clave de cada tema.\n",
        "ğŸš€ Â¡Este mÃ©todo es Ãºtil para clasificaciÃ³n automÃ¡tica de textos y anÃ¡lisis de contenido en NLP!\n",
        "\n",
        "\n",
        "* ğŸ—ï¸ Â¿CÃ³mo funciona LDA?\n",
        "LDA modela cada documento como una combinaciÃ³n de temas, y cada tema como una combinaciÃ³n de palabras. Funciona con los siguientes pasos:\n",
        "*\n",
        "1ï¸âƒ£ Convertir texto en datos estructurados\n",
        "\n",
        "Se crea una matriz de documentos vs palabras usando tÃ©cnicas como CountVectorizer (Bag of Words).\n",
        "* 2ï¸âƒ£ AsignaciÃ³n inicial de temas\n",
        "\n",
        "Asigna palabras a temas de manera aleatoria.\n",
        "* 3ï¸âƒ£ Ajuste iterativo\n",
        "\n",
        "Usa inferencia bayesiana para refinar la asignaciÃ³n de palabras a temas.\n",
        "Ajusta los pesos de cada palabra en cada tema hasta que se alcance un equilibrio.\n",
        "* 4ï¸âƒ£ ExtracciÃ³n de temas\n",
        "\n",
        "Devuelve una lista de temas con palabras representativas.\n",
        "* ğŸ§  Modelo MatemÃ¡tico de LDA\n",
        "LDA usa un modelo generativo basado en distribuciones de Dirichlet:\n",
        "\n",
        "ğŸ“Œ Conceptos clave:\n",
        "\n",
        "Un documento\n",
        "ğ‘‘\n",
        "d es una mezcla de\n",
        "ğ¾\n",
        "K temas.\n",
        "Un tema\n",
        "ğ‘§\n",
        "z es una distribuciÃ³n sobre palabras.\n",
        "Cada palabra en un documento se elige de acuerdo con la distribuciÃ³n del tema asignado.\n",
        "ğŸ”¢ FÃ³rmulas bÃ¡sicas:\n",
        "\n",
        "DistribuciÃ³n de temas en un documento:\n",
        "\n",
        "ğœƒ\n",
        "ğ‘‘\n",
        "âˆ¼\n",
        "ğ·\n",
        "ğ‘–\n",
        "ğ‘Ÿ\n",
        "ğ‘–\n",
        "ğ‘\n",
        "â„\n",
        "ğ‘™\n",
        "ğ‘’\n",
        "ğ‘¡\n",
        "(\n",
        "ğ›¼\n",
        ")\n",
        "Î¸\n",
        "d\n",
        "â€‹\n",
        " âˆ¼Dirichlet(Î±)\n",
        "Donde\n",
        "ğœƒ\n",
        "ğ‘‘\n",
        "Î¸\n",
        "d\n",
        "â€‹\n",
        "  es la probabilidad de cada tema en el documento\n",
        "ğ‘‘\n",
        "d, y\n",
        "ğ›¼\n",
        "Î± es un parÃ¡metro de Dirichlet.\n",
        "\n",
        "DistribuciÃ³n de palabras en un tema:\n",
        "\n",
        "ğœ™\n",
        "ğ‘˜\n",
        "âˆ¼\n",
        "ğ·\n",
        "ğ‘–\n",
        "ğ‘Ÿ\n",
        "ğ‘–\n",
        "ğ‘\n",
        "â„\n",
        "ğ‘™\n",
        "ğ‘’\n",
        "ğ‘¡\n",
        "(\n",
        "ğ›½\n",
        ")\n",
        "Ï•\n",
        "k\n",
        "â€‹\n",
        " âˆ¼Dirichlet(Î²)\n",
        "Donde\n",
        "ğœ™\n",
        "ğ‘˜\n",
        "Ï•\n",
        "k\n",
        "â€‹\n",
        "  representa la distribuciÃ³n de palabras dentro del tema\n",
        "ğ‘˜\n",
        "k, y\n",
        "ğ›½\n",
        "Î² es otro parÃ¡metro de Dirichlet.\n",
        "\n",
        "AsignaciÃ³n de palabras:\n",
        "\n",
        "Para cada palabra en un documento:\n",
        "Se elige un tema\n",
        "ğ‘§\n",
        "ğ‘‘\n",
        "ğ‘›\n",
        "z\n",
        "dn\n",
        "â€‹\n",
        "  segÃºn la distribuciÃ³n\n",
        "ğœƒ\n",
        "ğ‘‘\n",
        "Î¸\n",
        "d\n",
        "â€‹\n",
        " .\n",
        "Se elige una palabra\n",
        "ğ‘¤\n",
        "ğ‘‘\n",
        "ğ‘›\n",
        "w\n",
        "dn\n",
        "â€‹\n",
        "  de la distribuciÃ³n\n",
        "ğœ™\n",
        "ğ‘§\n",
        "ğ‘‘\n",
        "ğ‘›\n",
        "Ï•\n",
        "z\n",
        "dn\n",
        "â€‹\n",
        "\n",
        "â€‹\n",
        " .\n",
        "LDA ajusta los valores de\n",
        "ğœƒ\n",
        "Î¸ y\n",
        "ğœ™\n",
        "Ï• mediante algoritmos como Colapso de Gibbs Sampling o Variational Inference.\n",
        "\n",
        "ğŸ“Œ Ejemplo Visual\n",
        "Supongamos que aplicamos LDA a estos documentos:\n",
        "\n",
        "* 1ï¸âƒ£ \"Me gusta comer pizza y hamburguesas.\"\n",
        "* 2ï¸âƒ£ \"El fÃºtbol es mi deporte favorito.\"\n",
        "* 3ï¸âƒ£ \"La inteligencia artificial estÃ¡ revolucionando el mundo.\"\n",
        "\n",
        "LDA puede detectar dos temas:\n",
        "\n",
        "Tema 1 (Comida): pizza, hamburguesa, comer.\n",
        "Tema 2 (Deportes): fÃºtbol, deporte, favorito.\n",
        "Tema 3 (TecnologÃ­a): inteligencia, artificial, mundo."
      ],
      "metadata": {
        "id": "cHbe2V08DAoN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "\n",
        "\n",
        "# Ejemplo de documentos\n",
        "docs = [\"Comer pizza es mi pasatiempo favorito.\",\n",
        "        \"La inteligencia artificial es el futuro.\",\n",
        "        \"El cafÃ© colombiano es el mejor del mundo.\"]\n",
        "\n",
        "\n",
        "# Convertir documentos a una matriz de conteos de palabras\n",
        "vectorizer = CountVectorizer()\n",
        "doc_term_matrix = vectorizer.fit_transform(docs)\n",
        "\n",
        "\n",
        "# Aplicar LDA\n",
        "lda = LatentDirichletAllocation(n_components=2, random_state=42)\n",
        "lda.fit(doc_term_matrix)\n",
        "\n",
        "\n",
        "# Mostrar los temas\n",
        "for idx, topic in enumerate(lda.components_):\n",
        "    print(f\"Tema {idx}:\")\n",
        "    print([vectorizer.get_feature_names_out()[i] for i in topic.argsort()[-10:]])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pm4p1YahCm4F",
        "outputId": "c834769d-9396-4ed8-b3e0-1ba4be019a0f"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tema 0:\n",
            "['pizza', 'comer', 'mi', 'mundo', 'colombiano', 'cafÃ©', 'mejor', 'del', 'es', 'el']\n",
            "Tema 1:\n",
            "['comer', 'pizza', 'favorito', 'pasatiempo', 'artificial', 'futuro', 'inteligencia', 'la', 'el', 'es']\n"
          ]
        }
      ]
    }
  ]
}