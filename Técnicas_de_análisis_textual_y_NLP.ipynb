{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOatmtwrhlMVfWcIzrneZjg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/edwinmgallego/Best-README-Template/blob/main/T%C3%A9cnicas_de_an%C3%A1lisis_textual_y_NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import fitz  # PyMuPDF\n",
        "\n",
        "# üîπ Especifica la ruta del archivo PDF\n",
        "pdf_path = \"/ruta/del/archivo/tu_archivo.pdf\"  # Cambia esto por la ruta real\n",
        "\n",
        "# üîπ Abrir el PDF y extraer el texto\n",
        "doc = fitz.open(pdf_path)\n",
        "text = \"\\n\".join(page.get_text(\"text\") for page in doc)\n",
        "\n",
        "# üîπ Imprimir el texto extra√≠do\n",
        "print(text)\n"
      ],
      "metadata": {
        "id": "5MhJCxfCFhG9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Procesamiento de Lenguaje Natural (NLP)\n",
        "\n",
        "* El NLP combina la ling√º√≠stica computacional con modelos estad√≠sticos, de aprendizaje autom√°tico y aprendizaje profundo para permitir que las computadoras procesen y analicen grandes cantidades de datos de lenguaje natural.\n"
      ],
      "metadata": {
        "id": "yx09FH713Ds9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oa0d9p-n3C9z",
        "outputId": "003ab4b4-241e-4880-b32b-7a084c14a1a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hola', ',', '¬øc√≥mo', 'est√°s', '?', 'Espero', 'que', 'todo', 'bien', '.']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# importando NLTK (Natural Language Toolkit), una librer√≠a de Python para el procesamiento de lenguaje natural. Adem√°s, desde nltk.tokenize, importas word_tokenize, que se usa para dividir un texto en palabras (tokenizaci√≥n).\n",
        "\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "# Download the 'punkt_tab' resource\n",
        "#'punkt' es un paquete de NLTK que contiene datos necesarios para la tokenizaci√≥n de texto en ingl√©s y otros idiomas.\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "\n",
        "#Aqu√≠ word_tokenize(texto) divide el texto en palabras y signos de puntuaci√≥n, devolviendo una lista de \"tokens\".\n",
        "texto = \"Hola, ¬øc√≥mo est√°s? Espero que todo bien.\"\n",
        "tokens = word_tokenize(texto)\n",
        "print(tokens)\n",
        "\n",
        "#üîπ Observaciones:\n",
        "#El texto se ha separado en palabras y signos de puntuaci√≥n.\n",
        "#El signo ¬ø es un token independiente.\n",
        "#El punto final . tambi√©n es tratado como un token separado.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download es_core_news_sm # Download the Spanish language model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lZ5NbB8F7ICE",
        "outputId": "05f1537a-c91c-4831-af3b-c2ec5693a7ed"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting es-core-news-sm==3.7.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/es_core_news_sm-3.7.0/es_core_news_sm-3.7.0-py3-none-any.whl (12.9 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m12.9/12.9 MB\u001b[0m \u001b[31m62.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.0 in /usr/local/lib/python3.11/dist-packages (from es-core-news-sm==3.7.0) (3.7.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.0.12)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.15.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (4.67.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.10.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.1.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (75.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.5.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.27.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2025.1.31)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.11/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.20.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (7.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.17.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.1.2)\n",
            "Installing collected packages: es-core-news-sm\n",
            "Successfully installed es-core-news-sm-3.7.0\n",
            "\u001b[38;5;2m‚úî Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('es_core_news_sm')\n",
            "\u001b[38;5;3m‚ö† Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Lematizaci√≥n**"
      ],
      "metadata": {
        "id": "4hfL3Spb5_dF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Esto importa la biblioteca spaCy, que es una poderosa herramienta para el procesamiento del lenguaje natural (NLP).\n",
        "\n",
        "import spacy\n",
        "\n",
        "\n",
        "#Aqu√≠ se carga un modelo de spaCy en espa√±ol llamado es_core_news_sm. Este modelo tiene conocimiento sobre la gram√°tica del espa√±ol y permite realizar tareas como tokenizaci√≥n, lematizaci√≥n, an√°lisis gramatical, reconocimiento de entidades.\n",
        "\n",
        "nlp = spacy.load('es_core_news_sm') # Load the downloaded model\n",
        "doc = nlp(\"Estoy corriendo en el parque\")\n",
        "for token in doc:\n",
        "    print(token.text, token.lemma_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fWqDcg5S6JoZ",
        "outputId": "e82f0462-1f0a-46e6-9662-7fdec3b26335"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Estoy estar\n",
            "corriendo correr\n",
            "en en\n",
            "el el\n",
            "parque parque\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "\n",
        "\n",
        "stop_words = set(stopwords.words('spanish'))\n",
        "filtered_tokens = [word for word in tokens if not word in stop_words]\n",
        "print(filtered_tokens)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EyC-sgli9zwC",
        "outputId": "2935805b-a08a-476a-e846-3fb809b02435"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hola', ',', '¬øc√≥mo', '?', 'Espero', 'bien', '.']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "* TextBlob  es una biblioteca de procesamiento de texto que permite an√°lisis de sentimientos, correcci√≥n ortogr√°fica, traducci√≥n, etc.\n",
        "texto = TextBlob(\"Me encanta este lugar, es maravilloso.\")\n",
        "\n",
        "*  Se crea un objeto TextBlob con el texto \"Me encanta este lugar, es maravilloso.\".\n",
        "\n",
        "*  TextBlob analiza el texto y puede extraer informaci√≥n como polaridad y subjetividad.\n",
        "print(texto.sentiment)\n",
        "\n",
        "* texto.sentiment devuelve un objeto con dos valores:\n",
        "** polarity (polaridad): Un valor entre -1 y 1 que indica si el sentimiento es negativo (-1), neutro (0) o positivo (1).\n",
        "* * subjectivity (subjetividad): Un valor entre 0 y 1 que indica qu√© tan subjetivo es el texto (0 = objetivo, 1 = subjetivo)."
      ],
      "metadata": {
        "id": "8P6uxxkvA1kY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from textblob import TextBlob\n",
        "\n",
        "\n",
        "\n",
        "texto = TextBlob(\"Me encanta este lugar, es maravilloso.\")\n",
        "print(texto.sentiment)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QhRfVlm4AWDy",
        "outputId": "ed91f1bc-96b3-4a4e-a3f7-1c896210b2bc"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentiment(polarity=0.0, subjectivity=0.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* nlp es un modelo de spaCy previamente cargado (por ejemplo, nlp = spacy.load(\"es_core_news_sm\") si se usa el modelo en espa√±ol).\n",
        "\n",
        "* Se procesa el texto \"Apple est√° buscando comprar una startup del Reino Unido por mil millones de d√≥lares.\"\n",
        "doc es un objeto de spaCy que contiene el an√°lisis del texto, incluyendo tokens, entidades, dependencias gramaticales, etc.\n",
        "\n",
        "* * doc.ents contiene las entidades nombradas que spaCy ha identificado en el texto.\n",
        "* * El bucle for recorre cada entidad (ent) y muestra:\n",
        "ent.text: el fragmento del texto que corresponde a la entidad.\n",
        "ent.label_: la etiqueta de la entidad (tipo de entidad reconocida).\n",
        "\n",
        "* Explicaci√≥n de las etiquetas:\n",
        "ORG ‚Üí Organizaci√≥n (Apple)\n",
        "LOC ‚Üí Ubicaci√≥n (Reino Unido)\n",
        "MONEY ‚Üí Cantidad de dinero (mil millones de d√≥lares)\n",
        "Si no se detectan correctamente, puede ser necesario utilizar un modelo m√°s grande (es_core_news_md o es_core_news_lg) o ajustar el reconocimiento de entidades con entrenamiento adicional.\n",
        "\n",
        "Conclusi√≥n\n",
        "Este c√≥digo utiliza spaCy para extraer entidades nombradas (como nombres de empresas, lugares y cantidades de dinero) de un texto en espa√±ol y mostrarlas junto con su categor√≠a."
      ],
      "metadata": {
        "id": "9zHF2JzjB_LZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(\"Apple est√° buscando comprar una startup del Reino Unido por mil millones de d√≥lares.\")\n",
        "\n",
        "\n",
        "for ent in doc.ents:\n",
        "    print(ent.text, ent.label_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iuMvmnuQBdm2",
        "outputId": "25002133-07bb-4d63-8b59-761e914aa062"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Apple ORG\n",
            "Reino Unido LOC\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Modelado de temas"
      ],
      "metadata": {
        "id": "sTIKibdHCvl0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* CountVectorizer: Convierte texto en una matriz de conteo de palabras.\n",
        "LatentDirichletAllocation: Implementa el algoritmo LDA para la detecci√≥n de temas en textos.\n",
        "\n",
        "\n",
        "\n",
        "* n_components=2: Se extraer√°n dos temas principales de los documentos.\n",
        "random_state=42: Se fija una semilla para reproducibilidad.\n",
        "* fit(doc_term_matrix): Se entrena el modelo para identificar patrones de palabras que forman los temas.\n",
        "\n",
        "* lda.components_ contiene los pesos de las palabras en cada tema.\n",
        "topic.argsort()[-10:]: Obtiene las 10 palabras m√°s representativas del tema actual (ordenadas por importancia).\n",
        "\n",
        "* Resumen:\n",
        "Convierte texto a una matriz num√©rica.\n",
        "Usa LDA para extraer temas latentes.\n",
        "Imprime palabras clave de cada tema.\n",
        "üöÄ ¬°Este m√©todo es √∫til para clasificaci√≥n autom√°tica de textos y an√°lisis de contenido en NLP!\n",
        "\n",
        "\n",
        "* üèóÔ∏è ¬øC√≥mo funciona LDA?\n",
        "LDA modela cada documento como una combinaci√≥n de temas, y cada tema como una combinaci√≥n de palabras. Funciona con los siguientes pasos:\n",
        "*\n",
        "1Ô∏è‚É£ Convertir texto en datos estructurados\n",
        "\n",
        "Se crea una matriz de documentos vs palabras usando t√©cnicas como CountVectorizer (Bag of Words).\n",
        "* 2Ô∏è‚É£ Asignaci√≥n inicial de temas\n",
        "\n",
        "Asigna palabras a temas de manera aleatoria.\n",
        "* 3Ô∏è‚É£ Ajuste iterativo\n",
        "\n",
        "Usa inferencia bayesiana para refinar la asignaci√≥n de palabras a temas.\n",
        "Ajusta los pesos de cada palabra en cada tema hasta que se alcance un equilibrio.\n",
        "* 4Ô∏è‚É£ Extracci√≥n de temas\n",
        "\n",
        "Devuelve una lista de temas con palabras representativas.\n",
        "* üß† Modelo Matem√°tico de LDA\n",
        "LDA usa un modelo generativo basado en distribuciones de Dirichlet:\n",
        "\n",
        "üìå Conceptos clave:\n",
        "\n",
        "Un documento\n",
        "ùëë\n",
        "d es una mezcla de\n",
        "ùêæ\n",
        "K temas.\n",
        "Un tema\n",
        "ùëß\n",
        "z es una distribuci√≥n sobre palabras.\n",
        "Cada palabra en un documento se elige de acuerdo con la distribuci√≥n del tema asignado.\n",
        "üî¢ F√≥rmulas b√°sicas:\n",
        "\n",
        "Distribuci√≥n de temas en un documento:\n",
        "\n",
        "ùúÉ\n",
        "ùëë\n",
        "‚àº\n",
        "ùê∑\n",
        "ùëñ\n",
        "ùëü\n",
        "ùëñ\n",
        "ùëê\n",
        "‚Ñé\n",
        "ùëô\n",
        "ùëí\n",
        "ùë°\n",
        "(\n",
        "ùõº\n",
        ")\n",
        "Œ∏\n",
        "d\n",
        "‚Äã\n",
        " ‚àºDirichlet(Œ±)\n",
        "Donde\n",
        "ùúÉ\n",
        "ùëë\n",
        "Œ∏\n",
        "d\n",
        "‚Äã\n",
        "  es la probabilidad de cada tema en el documento\n",
        "ùëë\n",
        "d, y\n",
        "ùõº\n",
        "Œ± es un par√°metro de Dirichlet.\n",
        "\n",
        "Distribuci√≥n de palabras en un tema:\n",
        "\n",
        "ùúô\n",
        "ùëò\n",
        "‚àº\n",
        "ùê∑\n",
        "ùëñ\n",
        "ùëü\n",
        "ùëñ\n",
        "ùëê\n",
        "‚Ñé\n",
        "ùëô\n",
        "ùëí\n",
        "ùë°\n",
        "(\n",
        "ùõΩ\n",
        ")\n",
        "œï\n",
        "k\n",
        "‚Äã\n",
        " ‚àºDirichlet(Œ≤)\n",
        "Donde\n",
        "ùúô\n",
        "ùëò\n",
        "œï\n",
        "k\n",
        "‚Äã\n",
        "  representa la distribuci√≥n de palabras dentro del tema\n",
        "ùëò\n",
        "k, y\n",
        "ùõΩ\n",
        "Œ≤ es otro par√°metro de Dirichlet.\n",
        "\n",
        "Asignaci√≥n de palabras:\n",
        "\n",
        "Para cada palabra en un documento:\n",
        "Se elige un tema\n",
        "ùëß\n",
        "ùëë\n",
        "ùëõ\n",
        "z\n",
        "dn\n",
        "‚Äã\n",
        "  seg√∫n la distribuci√≥n\n",
        "ùúÉ\n",
        "ùëë\n",
        "Œ∏\n",
        "d\n",
        "‚Äã\n",
        " .\n",
        "Se elige una palabra\n",
        "ùë§\n",
        "ùëë\n",
        "ùëõ\n",
        "w\n",
        "dn\n",
        "‚Äã\n",
        "  de la distribuci√≥n\n",
        "ùúô\n",
        "ùëß\n",
        "ùëë\n",
        "ùëõ\n",
        "œï\n",
        "z\n",
        "dn\n",
        "‚Äã\n",
        "\n",
        "‚Äã\n",
        " .\n",
        "LDA ajusta los valores de\n",
        "ùúÉ\n",
        "Œ∏ y\n",
        "ùúô\n",
        "œï mediante algoritmos como Colapso de Gibbs Sampling o Variational Inference.\n",
        "\n",
        "üìå Ejemplo Visual\n",
        "Supongamos que aplicamos LDA a estos documentos:\n",
        "\n",
        "* 1Ô∏è‚É£ \"Me gusta comer pizza y hamburguesas.\"\n",
        "* 2Ô∏è‚É£ \"El f√∫tbol es mi deporte favorito.\"\n",
        "* 3Ô∏è‚É£ \"La inteligencia artificial est√° revolucionando el mundo.\"\n",
        "\n",
        "LDA puede detectar dos temas:\n",
        "\n",
        "Tema 1 (Comida): pizza, hamburguesa, comer.\n",
        "Tema 2 (Deportes): f√∫tbol, deporte, favorito.\n",
        "Tema 3 (Tecnolog√≠a): inteligencia, artificial, mundo."
      ],
      "metadata": {
        "id": "cHbe2V08DAoN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "\n",
        "\n",
        "# Ejemplo de documentos\n",
        "docs = [\"Comer pizza es mi pasatiempo favorito.\",\n",
        "        \"La inteligencia artificial es el futuro.\",\n",
        "        \"El caf√© colombiano es el mejor del mundo.\"]\n",
        "\n",
        "\n",
        "# Convertir documentos a una matriz de conteos de palabras\n",
        "vectorizer = CountVectorizer()\n",
        "doc_term_matrix = vectorizer.fit_transform(docs)\n",
        "\n",
        "\n",
        "# Aplicar LDA\n",
        "lda = LatentDirichletAllocation(n_components=2, random_state=42)\n",
        "lda.fit(doc_term_matrix)\n",
        "\n",
        "\n",
        "# Mostrar los temas\n",
        "for idx, topic in enumerate(lda.components_):\n",
        "    print(f\"Tema {idx}:\")\n",
        "    print([vectorizer.get_feature_names_out()[i] for i in topic.argsort()[-10:]])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pm4p1YahCm4F",
        "outputId": "c834769d-9396-4ed8-b3e0-1ba4be019a0f"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tema 0:\n",
            "['pizza', 'comer', 'mi', 'mundo', 'colombiano', 'caf√©', 'mejor', 'del', 'es', 'el']\n",
            "Tema 1:\n",
            "['comer', 'pizza', 'favorito', 'pasatiempo', 'artificial', 'futuro', 'inteligencia', 'la', 'el', 'es']\n"
          ]
        }
      ]
    }
  ]
}