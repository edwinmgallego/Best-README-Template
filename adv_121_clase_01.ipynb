{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPleDVqlGRXAO9AnM4lf0wz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/edwinmgallego/Best-README-Template/blob/main/adv_121_clase_01.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9bsDBAv7k_GN"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "# Cargar dataset de ejemplo\n",
        "df = pd.read_csv('titanic.csv')\n",
        "# Resumen de los datos\n",
        "print(df.info())\n",
        "print(df.describe())\n",
        "print(df.head())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Manejo de valores faltantes\n",
        "df.fillna(df.mean(), inplace=True)\n",
        "# Detección de outliers con IQR\n",
        "Q1 = df.quantile(0.25)\n",
        "Q3 = df.quantile(0.75)\n",
        "IQR = Q3 - Q1\n",
        "outliers = ((df < (Q1 - 1.5 * IQR)) | (df > (Q3 + 1.5 * IQR))).sum()\n",
        "print(outliers)"
      ],
      "metadata": {
        "id": "6H9BJ8ghlXL2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Transformación de Datos:\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "scaler = MinMaxScaler()\n",
        "df[['Age', 'Fare']] = scaler.fit_transform(df[['Age', 'Fare']])"
      ],
      "metadata": {
        "id": "649k-uTalgS6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## reduccion de  dimencionalidad\n",
        "from sklearn.decomposition import PCA\n",
        "pca = PCA(n_components=2)\n",
        "df_pca = pca.fit_transform(df[['Age', 'Fare']])"
      ],
      "metadata": {
        "id": "6Ql6VmKTllGT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Exploración de Datos (EDA)"
      ],
      "metadata": {
        "id": "BH9wdjOPmBPz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#tecnica de mineria de datos   modelos  supervisados\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "X = df[['Pclass', 'Age', 'Fare']]\n",
        "y = df['Survived']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "model = RandomForestClassifier()\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "print(f'Precisión: {accuracy_score(y_test, y_pred)}')"
      ],
      "metadata": {
        "id": "2sBn5z1EmC-0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##2.2 Agrupamiento (Clustering) (1 hora)\n",
        "from sklearn.cluster import KMeans\n",
        "X = df[['Age', 'Fare']]\n",
        "kmeans = KMeans(n_clusters=3)\n",
        "df['Cluster'] = kmeans.fit_predict(X)\n",
        "sns.scatterplot(x=df['Age'], y=df['Fare'], hue=df['Cluster'])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Fq4DHW0kmfds"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##patrones por asociacion\n",
        "\n",
        "from mlxtend.frequent_patterns import apriori, association_rules\n",
        "from mlxtend.preprocessing import TransactionEncoder\n",
        "\n",
        "#mlxtend.frequent_patterns.apriori: Esta función implementa el algoritmo de Apriori, utilizado para encontrar itemsets frecuentes en conjuntos de datos.\n",
        "#mlxtend.frequent_patterns.association_rules: Esta función genera reglas de asociación a partir de los itemsets frecuentes,\n",
        "#basándose en diferentes métricas, como la confianza.\n",
        "#mlxtend.preprocessing.TransactionEncoder: Esta clase convierte las transacciones (listas de artículos)\n",
        "#en un formato adecuado para aplicar el algoritmo de Apriori.\n",
        "transactions = [['leche', 'pan'], ['pan', 'mantequilla'], ['leche', 'mantequilla', 'pan']]\n",
        "\n",
        "#Este es un ejemplo de conjunto de transacciones. Cada transacción es una lista de productos comprados juntos en una misma compra.\n",
        "\n",
        "#Transacción 1: ['leche', 'pan']\n",
        "#Transacción 2: ['pan', 'mantequilla']\n",
        "#Transacción 3: ['leche', 'mantequilla', 'pan']\n",
        "\n",
        "\n",
        "#Aquí se crea una instancia de TransactionEncoder y se transforma la lista de transacciones en una matriz binaria.\n",
        "\n",
        "#fit(transactions): Analiza las transacciones y aprende sobre los datos.\n",
        "#transform(transactions): Convierte las transacciones a una matriz binaria,\n",
        "#donde cada columna representa un artículo y cada fila una transacción. Si un artículo aparece en una transacción, se marca como 1, de lo contrario 0.\n",
        "#te = TransactionEncoder()\n",
        "te_ary = te.fit(transactions).transform(transactions)\n",
        "\n",
        "#Aquí se crea un DataFrame de Pandas a partir de la matriz binaria, donde las columnas representan los artículos\n",
        "# y las filas las transacciones. El valor en cada celda es 1 si el artículo está presente en la transacción y 0 si no.\n",
        "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
        "#Se aplica el algoritmo de Apriori para encontrar los itemsets frecuentes, con un soporte mínimo de 0.5\n",
        " #(es decir, un itemset debe aparecer en al menos el 50% de las transacciones).\n",
        "\n",
        "#df: El DataFrame con la matriz binaria de transacciones.\n",
        "#min_support=0.5: El soporte mínimo requerido para que un conjunto de elementos sea considerado frecuente.\n",
        "#use_colnames=True: Utiliza los nombres de los artículos como nombres de las columnas.\n",
        "frequent_itemsets = apriori(df, min_support=0.5, use_colnames=True)\n",
        "#Se generan reglas de asociación a partir de los itemsets frecuentes utilizando la métrica de confianza (confidence).\n",
        "#La confianza es una medida que indica la probabilidad de que el antecedente de la regla implique el consecuente.\n",
        "\n",
        "#metric=\"confidence\": La métrica utilizada para generar las reglas es la confianza.\n",
        "#min_threshold=0.5: El umbral mínimo de confianza para las reglas generadas. Solo se seleccionarán las reglas cuya confianza sea al menos del 50%.\n",
        "#\n",
        "rules = association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=0.5)\n",
        "#Finalmente, se imprimen las reglas de asociación generadas.\n",
        "# Estas reglas indican relaciones entre productos basadas en su co-ocurrencia en las transacciones.\n",
        "\n",
        "#Ejemplo de lo que podría imprimir:\n",
        "#Imaginemos que los itemsets frecuentes encontrados son:\n",
        "\n",
        "#{leche, pan} con soporte 0.67\n",
        "#{pan, mantequilla} con soporte 0.67\n",
        "#Y las reglas generadas podrían ser, por ejemplo:\n",
        "\n",
        "#Regla 1: Si se compra leche, entonces también se compra pan con una confianza del 100%.\n",
        "#Regla 2: Si se compra pan, entonces también se compra mantequilla con una confianza del 100%.\n",
        "#Este es un ejemplo básico, pero el algoritmo de Apriori se puede aplicar a conjuntos de datos mucho más grandes para descubrir patrones de compra frecuentes.\n",
        "\n",
        "#\n",
        "\n",
        "print(rules)"
      ],
      "metadata": {
        "id": "4MuVqzMZmxTL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Cargar dataset (descargar 'train.csv' desde Kaggle y colocarlo en la misma carpeta)\n",
        "df = pd.read_csv(\"train.csv\")\n",
        "\n",
        "# Mostrar información general\n",
        "print(df.info())\n",
        "\n",
        "# Resumen estadístico\n",
        "print(df.describe())\n",
        "\n",
        "# Ver valores faltantes\n",
        "print(df.isnull().sum())\n",
        "\n",
        "# Visualizar distribución de supervivientes\n",
        "sns.countplot(x='Survived', data=df)\n",
        "plt.title(\"Distribución de Supervivientes\")\n",
        "plt.show()\n",
        "\n",
        "# Comparación de supervivencia según el sexo\n",
        "sns.barplot(x=\"Sex\", y=\"Survived\", data=df)\n",
        "plt.title(\"Supervivencia según Género\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "OaOZtbwQu9tu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Clasificación de dígitos manuscritos con k-NN\n",
        "#Objetivo: Usar k-Nearest Neighbors (k-NN) para clasificar imágenes del dataset MNIST.\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Cargar dataset\n",
        "df = pd.read_csv(\"train.csv\")\n",
        "\n",
        "# Separar características (X) y etiquetas (y)\n",
        "X = df.drop(columns=['label']).values\n",
        "y = df['label'].values\n",
        "\n",
        "# Dividir en conjunto de entrenamiento y prueba\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Normalizar los datos\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Entrenar modelo k-NN\n",
        "knn = KNeighborsClassifier(n_neighbors=5)\n",
        "knn.fit(X_train, y_train)\n",
        "\n",
        "# Hacer predicciones\n",
        "y_pred = knn.predict(X_test)\n",
        "\n",
        "# Evaluar precisión\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Precisión del modelo k-NN: {accuracy:.4f}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "5gB2hVQwwCbW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Agrupamiento de clientes con K-Means en datos de compras\n",
        "#Objetivo: Aplicar clustering en un dataset de clientes para segmentar el mercado.\n",
        "\n",
        "#Dataset: Mall Customers Segmentation Data\n",
        "\n",
        "#Este código realiza agrupamiento de clientes utilizando el algoritmo K-Means en un dataset llamado Mall Customers Segmentation Data.\n",
        "#Su objetivo es segmentar el mercado en base al ingreso anual y el puntaje de gasto de los clientes.\n",
        "\n",
        "#A continuación, explico paso a paso:\n",
        "#pandas: Para manejar el dataset.\n",
        "#matplotlib.pyplot: Para graficar los resultados.\n",
        "#KMeans de sklearn.cluster: Para aplicar el algoritmo de agrupamiento.\n",
        "#StandardScaler de sklearn.preprocessing: Para normalizar los datos.\"\"\n",
        "\n",
        "#\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Cargar dataset\n",
        "#Se carga el archivo Mall_Customers.csv en un DataFrame de pandas.\n",
        "df = pd.read_csv(\"Mall_Customers.csv\")\n",
        "\n",
        "# Seleccionar las variables relevantes para el clustering\n",
        "#e extraen dos columnas:\n",
        "#Annual Income (k$) → Ingreso anual del cliente en miles de dólares.\n",
        "#Spending Score (1-100) → Un puntaje de gasto de 1 a 100.\n",
        "X = df[['Annual Income (k$)', 'Spending Score (1-100)']]\n",
        "\n",
        "# Normalizar datos\n",
        "#Se usa StandardScaler para escalar los datos a una distribución con media 0 y desviación estándar 1.\n",
        "#Esto es importante para que K-Means funcione mejor al evitar que una variable tenga más peso que otra.\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Aplicar el método del codo para encontrar el número óptimo de clusters\n",
        "\n",
        "#Se prueban valores de k (número de clusters) desde 1 hasta 10.\n",
        "#Se mide la inercia (distancia interna entre puntos de un cluster).\n",
        "#Se guarda el resultado en la lista inertia.\n",
        "inertia = []\n",
        "for k in range(1, 11):\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
        "    kmeans.fit(X_scaled)\n",
        "    inertia.append(kmeans.inertia_)\n",
        "\n",
        "# Graficar el método del codo\n",
        "#Se grafica inertia contra el número de clusters (k).\n",
        "#El \"codo\" en la gráfica indica el número óptimo de clusters.\n",
        "plt.plot(range(1, 11), inertia, marker='o')\n",
        "plt.xlabel('Número de clusters')\n",
        "plt.ylabel('Inercia')\n",
        "plt.title('Método del Codo')\n",
        "plt.show()\n",
        "\n",
        "# Entrenar modelo K-Means con k=5 (según la gráfica del codo)\n",
        "#Se elige k=5 basándose en el codo de la gráfica.\n",
        "#Se aplica fit_predict() para entrenar y asignar un cluster a cada cliente.\n",
        "kmeans = KMeans(n_clusters=5, random_state=42)\n",
        "df['Cluster'] = kmeans.fit_predict(X_scaled)\n",
        "\n",
        "# Visualizar los clusters\n",
        "#Se genera un scatter plot con los puntos coloreados según su cluster.\n",
        "#El color representa a qué grupo pertenece cada cliente.\n",
        "plt.scatter(df['Annual Income (k$)'], df['Spending Score (1-100)'], c=df['Cluster'], cmap='viridis', alpha=0.7)\n",
        "plt.xlabel('Ingreso Anual (k$)')\n",
        "plt.ylabel('Puntaje de Gasto')\n",
        "plt.title('Segmentación de Clientes')\n",
        "plt.colorbar(label='Cluster')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "MKuCNOD-wTfT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}